<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Science, meet productivity, A computational RNA biologist exploring productivity, python, and reproducibility.">


        <title>1, 2, 3 … logistic regression! // Science, meet productivity // A computational RNA biologist exploring productivity, python, and reproducibility.</title>

    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.3.0/pure-min.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.1.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="../../../../../theme/css/pure.css">
    <link rel="stylesheet" href="../../../../../theme/css/pygments.css">

    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/fitvids/1.0.1/jquery.fitvids.min.js"></script>
    <script>
        $(document).ready(function(){
            $(".content").fitVids();
        });
    </script>
</head>

<body>
    <div class="pure-g-r" id="layout">
        <div class="sidebar pure-u">
            <div class="cover-img" style="background-image: url('https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/ordered_driftwood.jpg')">
                <div class="cover-body">
                    <header class="header">
                        <hgroup>
                            <img class="avatar" src="http://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/olga_icon_square.jpg">
                            <h1 class="brand-main"><a href="/">Science, meet productivity</a></h1>
                            <p class="tagline">A computational RNA biologist exploring productivity, python, and reproducibility.</p>
                                <p class="links"><a href="../../../../../archives.html">Archive</a></p>
                                <p class="links"><a href="../../../../../pages/about.html">About</a></p>
                                <p class="social">
                                    <a href="http://github.com/olgabot">
                                        <i class="fa fa-github-square fa-3x"></i>
                                    </a>
                                    <a href="http://twitter.com/olgabot">
                                        <i class="fa fa-twitter-square fa-3x"></i>
                                    </a>
                                    <a href="http://www.linkedin.com/in/olgabotvinnik/">
                                        <i class="fa fa-linkedin-square fa-3x"></i>
                                    </a>
                                <p>
                        </hgroup>
                    </header>
                </div>
            </div>
        </div>
    <div class="pure-u">
        <div class="content">
            <section class="post">
                <header class="post-header">
                    <h1>1, 2, 3 &#8230; logistic&nbsp;regression!</h1>
                        <p class="post-meta">
                            // under                                 <a class="post-category" href="../../../../../tag/r.html">R</a>
                                <a class="post-category" href="../../../../../tag/logistic-regression.html">logistic regression</a>
                                <a class="post-category" href="../../../../../tag/tutorial.html">tutorial</a>
                                <a class="post-category" href="../../../../../tag/star-trek.html">star trek</a>
                        </p>
                </header>
            </section>
            <h2>R Tutorial in Logistic&nbsp;Regression</h2>
<p>Recently I read a <a href="http://www.nature.com/msb/journal/v3/n1/full/msb4100180.html">paper</a> 
that used logistic regression in its methods
and realized I had no idea what logistic regression <em>is</em>, so I made this tutorial
for myself and others to learn about it. I wanted to create a tutorial that created 
random data that anyone could use, and taught you a bit about R in the process. This 
is my first tutorial and feedback is&nbsp;encouraged.</p>
<p>This tutorial on the statistical machine-learning method known as Logistic
Regression is implemented in <a href="r-project.org">R</a>. Familiarity with programming
syntax, such as vector indexing, is assumed but I tried to make this tutorial
as accessible as possible for those with less programming&nbsp;experience.</p>
<h1>What in Merlin&#8217;s beard is <em>logistic&nbsp;regression?</em></h1>
<h2>First, what is <em>regression</em>?</h2>
<p>You may have heard of &#8220;linear regression,&#8221; which fits a $y=mx+b$ line to a set of points, such as in the figure below. I&#8217;m not going to go into the details of linear regression, but you can <a href="http://www.stat.yale.edu/Courses/1997-98/101/linreg.htm">read about it</a> if you&nbsp;like.</p>
<p><img alt="Linear regression matches a best-fitting line to a set of data." src="http://media.tumblr.com/tumblr_mdorakn4cw1rw6gvj.png" /></p>
<!-- Add description of Logit function -->

<p>Logistic regression is similar, but instead of having two continuous variables as above, one of the variables is binary (0 or 1) and the other is continuous. But we&#8217;re not fitting a line, we&#8217;re fitting a <em>logit</em>, which is a totally different function that estimates probability for a continuous variable rather than assigns classification. For example, we could predict the probability
of cancer relapse given some gene expression values. Or the probability of a heart attack
given a person&#8217;s cholesterol levels. We would base these probabilities on
previously encountered data, a method of machine learning known as 
&#8220;Supervised Learning.&#8221; You tell the computer what the answer is for many examples, and then the computer has to guess whether the outcome is 0 or&nbsp;1.</p>
<p>Think of it this way: you look at hundreds of values representing the number of 
<em>Star Trek: The Next Generation</em> episodes a person has watched, with a teacher 
peering over your shoulder, and he tells you whether each value is associated 
with attending a <em>Star Trek</em> convention or not. Then the teacher leaves, and
you&#8217;re given a new number of <em>Star Trek: <span class="caps">TNG</span></em> episodes, $x$, and you have to give a 
probability $p$ whether the person who has seen this many episodes has been to a 
convention. The probability of $x$ episodes watched and no convention attendance
is $1-p$, so by knowing $p$, we also know $1-p$ and we only need to specify one&nbsp;value.</p>
<h2>Regression is not the same thing as&nbsp;classification</h2>
<p>This is an important distinction between classification and regression. If we were
doing classification, we would just say whether the person who has seen $x$ number 
of episodes has gone to a convention or not. But
we don&#8217;t always want a binary outcome. The cholesterol-heart attack association is a 
great example of this. You probably don&#8217;t want to tell a patient, &#8220;You <em>will</em> have a 
heart attack,&#8221; but rather say &#8220;You have an $x$% probability of having a heart
attack, given your current cholesterol levels.&#8221; Same goes for cancer relapse. A
binary yes/no answer can obscure the data. Also, a probability of 0 is essentially 
impossible. Could you truly, definitively, say that a person will <em>never</em> have a 
heart attack? Not until their death, but then all possible heart attack events have
already been observed. Sorry for the&nbsp;gruesome-ness.</p>
<h2>So what about this <em>logisitic</em>&nbsp;thing?</h2>
<h3>First, we have to talk about <em>odds</em></h3>
<p>We talked about predicting the probability of some event. But we&#8217;re going to use 
<em>odds</em> instead. If the probability of having attended a <em>Star Trek: <span class="caps">TNG</span></em> convention
is $p$, then the odds&nbsp;are:</p>
<p>$$\text{odds} = \frac{\text{probability of event ocurring}}{\text{probability of event not occuring}} =&nbsp;\frac{p}{1-p}$$</p>
<p>Say after watching a certain number of episodes, the probability of attending a 
convention is $p=0.7$. Then the odds of attending a convention&nbsp;are </p>
<p>$$\frac{0.7}{0.3} =&nbsp;2.333\ldots$$</p>
<p>But the odds of not attending a convention&nbsp;are </p>
<p>$$\frac{0.3}{0.7} =&nbsp;0.4285\ldots$$ </p>
<p>As you can see, odds and probability are not the same. Odds can be greater than 1, and probability must be between 0 and&nbsp;1. </p>
<h3>Next, we have to define <em>log-odds</em></h3>
<p>It would be nice if these odds were somehow symmetric and we could intuitively make 
inferences just by looking at the value. This is where the logarithm comes in. If we 
take the <a href="http://en.wikipedia.org/wiki/Natural_logarithm">natural log</a> (ln), aka 
$\log_e$ of the odds of attending a convention, we&nbsp;get</p>
<p>$$\ln \frac{0.7}{0.3} =&nbsp;0.8472\ldots$$</p>
<p>And the log-odds of <em>not</em> attending a&nbsp;convention:</p>
<p>$$\frac{0.3}{0.7} =&nbsp;-0.8472\ldots$$ </p>
<p>The opposite of attending a convention is the negative of attending a convention! 
Now let&#8217;s stop and think. <em>Why</em> is this? This is because the natural log of 1 is 0,
so it intersects at 0 as in the plot below. Then the natural log of anything smaller 
than 1 is negative, approaching negative infinity as the number gets closer and 
closer to 0. The natural log of anything larger than 1 is positive, and approaches 
positive infinity as the number gets larger and larger, but slower than we approach 
negative infinity from the other&nbsp;side. </p>
<p><img alt="Natural log function" src="http://media.tumblr.com/tumblr_mdorb7uHwT1rw6gvj.png" /></p>
<p>This symmetry is helpful to guide our intuition about events occurring or not&nbsp;occuring.</p>
<p><em><span class="caps">FYI</span>:</em> We used the natural logarithm here because that&#8217;s the standard. You could
really use any log base you like, even $\log_\pi$ would do, but it&#8217;s just not
used that&nbsp;often.</p>
<h3>From log-odds to <em>logits</em></h3>
<p>The log-odds are also called the <em>logit</em> of the&nbsp;probability:</p>
<p>$$ \log(\text{odds}) = \text{logit}(p) = \ln\left( \frac{p}{1-p} \right)&nbsp;$$</p>
<p>In logistic regression, we&nbsp;set </p>
<p>$$\text{logit}(p) = a_0 + a_1x_1 + a_2x_2, \ldots ,&nbsp;a_nx_n$$.</p>
<p>Where $n$ is the number of independent variables you have. In our case, we just have one independent variable, $x = x_1$, the number of <em>Star Trek: <span class="caps">TNG</span></em> episodes viewed. But we could add to the complexity and also have $x_2$ be the number of <em>Star Trek: The Original Series</em> (<a href="http://mashable.com/2012/09/07/google-star-trek/">happy 46th anniversary!</a>) episodes viewed, $x_3$ be number of <a href="http://en.memory-alpha.org/wiki/Latinum">latinum strips</a> owned, and $x_4$ be <a href="http://www.amazon.com/Star-Trek-Conversational-Marc-Okrand/dp/0671797395">Klingon language proficiency</a>. But we&#8217;re going to keep it simple and stick to our one&nbsp;$x$.</p>
<p>Now we could do regular ol&#8217; linear regression on this, but there are several&nbsp;issues.</p>
<ol>
<li>We have a binary variable for our dependent variable (aka the $y$), and linear regression assumes a continuous distribution for the dependent variable. With probability as the output, we cannot have values less than 0 or greater than 1, and linear regression does not conform to&nbsp;this.</li>
<li>The variance of $y$ is not constant across values of $x$. The variance is $p(1-p) = pq$ ($q$ is short for $1-p$), and if we have a probability of $0.50$ of attending a <em>Star Trek</em> convention, then we have $0.5\times 0.5 = 0.25$ odds. But if we have $p=0.9$, then the variance $pq = 0.9\times 0.1 = 0.09$, and $0.25 \neq 0.09$. Thus the assumption that variance of $y$ is constant across values of $x$ is invalid. This is also called <a href="http://blog.minitab.com/blog/statistics-and-quality-data-analysis/dont-be-a-victim-of-statistical-hippopotomonstrosesquipedaliophobia">homoscedasticity</a>. </li>
<li>If you want to do significance testing (you maybe familiar with <em>p</em>-values or <em>R</em>-values to describe how significant a result is), the assumption that the errors of prediction, $Y-Y^\prime$ are <a href="http://en.wikipedia.org/wiki/Normal_distribution">normally distributed</a> doesn&#8217;t work. $Y$ only takes on the values 0 and 1, so there&#8217;s no way you can create a continuous distribution such as a normal distribution with just two&nbsp;values.</li>
</ol>
<p>Logits are not very intuitive as a unit, even though they are very helpful when looking at odds. But we need to get back to probabilities so we can give a probability of convention attendance. We&#8217;ll have to exponentiate both sides to get rid of the natural log. I&#8217;m going to keep the long-form version of the equation so you can see how it works for more&nbsp;variables.</p>
<p>$$
\begin{align}
\text{logit}(p) &amp;= \ln\left( \frac{p}{1-p} \right) = a_0 + a_1x_1 + a_2x_2, \ldots , a_nx_n\newline
\frac{p}{1-p} &amp;= e^{a_0 + a_1x_1 + a_2x_2, \ldots , a_nx_n} \newline
p &amp;= (1-p)e^{a_0 + a_1x_1 + a_2x_2, \ldots , a_nx_n} \newline
p + p(e^{-(a_0 + a_1x_1 + a_2x_2, \ldots , a_nx_n)}) &amp;= 1 \newline
1 + e^{`(a_0 + a_1x_1 + a_2x_2, \ldots , a_nx_n)} &amp;= \frac{1}{p}\newline
\frac{1}{1+e^{-(a_0 + a_1x_1 + a_2x_2, \ldots , a_nx_n)}} &amp;= p
\end{align}&nbsp;$$</p>
<p>We can multiply the fraction by a clever form of 1, $e^{(a_0 + a_1x_1 + a_2x_2, \ldots , a_nx_n)}/e^{(a_0 + a_1x_1 + a_2x_2, \ldots , a_nx_n)}$ to&nbsp;get</p>
<p>$$
p = \frac{e^{(a_0 + a_1x_1 + a_2x_2, \ldots , a_nx_n)}}{1+e^{(a_0 + a_1x_1 + a_2x_2, \ldots , a_nx_n)}}.&nbsp;$$</p>
<p>If we set $a_1 = 1$ and all other $a_0, a_2, a_3, \ldots = 0$, then we get the general logit cuve&nbsp;below</p>
<p><img alt="The logit curve ranges from negative infinity to positive infinity on the x-axis, and from 0 to 1 on the y-axis." src="http://media.tumblr.com/tumblr_mdorbq7Ixx1rw6gvj.png" /></p>
<p>So now we have a way of creating a predictor of the probability of a binary variable from a continuous variable. Now let&#8217;s get started with doing&nbsp;regression.</p>
<p><em><span class="caps">PS</span>, if you want to get math typesetting (aka LaTeX) on your blog, I recommend 
<strong>against</strong> searching for &#8220;tumblr latex&#8221; and instead checking out <a href="http://checkmyworking.com/2012/01/how-to-get-beautifully-typeset-maths-on-your-blog/">this tutorial</a> on installing MathJax on your blog, tumblr or&nbsp;not.</em></p>
<h1>Step 1: Get binary&nbsp;data.</h1>
<h2>Initialize a vector of&nbsp;0&#8217;s</h2>
<p>For this case, we are going to make up some binary data. According to <a href="http://www.statgun.com/tutorials/logistic-regression.html">a nice explanation of logistic regression</a>,
logistic regression requires at least 50 data points per classification. Meaning, 
50+ 0&#8217;s and 50+ 1&#8217;s. There are 178 total <em>Star Trek: <span class="caps">TNG</span></em> episodes and lots of bonus 
features and interviews on the DVDs, so let&#8217;s round up to 200 total <em><span class="caps">TNG</span></em> pieces of 
video. In R, <code>rep</code> is the function to create a vector of repeated values. The
first argument is what you want repeated, and the second argument is how
many times you want to repeat it. You could even have a vector in the first 
argument, such as <code>rep(c("red", "blue"), 30)</code>, to get a vector of alternating 
strings, &#8220;red&#8221; and &#8220;blue.&#8221; We could have also made the same vector
with <code>classification = vector(length=200, mode="numeric")</code>, which initializes
the numeric values at 0, but I decided to do it with <code>rep</code> instead.</p>
<div class="highlight"><pre><span class="n">classification</span> <span class="o">=</span> <span class="n">rep</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">200</span><span class="p">)</span>
</pre></div>


<p><span class="caps">EDIT</span>: I&#8217;ve been informed that at 50+ data points is not always necessary, so take the rule with a grain of&nbsp;salt.</p>
<h2>Set a random&nbsp;seed</h2>
<p>Let&#8217;s set the random seed so we will get consistently the same results, no
matter how many times we run the code. No random number generator is truly random,
and part of the semi-randomness is starting with a seed number, and then doing 
all kinds of transformations to then get a &#8220;random&#8221; number. So if we make sure
we are always using the same seed, then we will always get the same set of
&#8220;random&#8221;&nbsp;numbers.</p>
<div class="highlight"><pre><span class="n">set</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
</pre></div>


<h2>Randomly add 1s to the last part of the&nbsp;vector</h2>
<p>Now let&#8217;s randomly add a bunch of 1s to the vector, ramping up the probability of 
getting a one (with hyperbolic growth, if you&#8217;re being technical) as we get to 
the end of the vector. Let&#8217;s be interesting and add some noise to the data so 
we don&#8217;t have not do an equal 50/50 split. We&#8217;ll use a randomly-generated
probability distribution to assign 0 or 1 to the last 125 values. This way, our
data is a little more realistic and less&nbsp;clean.</p>
<p><span class="caps">FYI</span>, <code>runif</code> is the <a href="http://en.wikipedia.org/wiki/Uniform_distribution_(continuous)">uniform distribution function</a>
in R, which has an equal probability of obtaining every value between 0 and 1.
It is useful in this assignment because I know that the odds are 50/50 that
the value will be greater than 0.5, so I increased the threshold to &gt;0.6
and added a multiplicative factor, which varies from 1 to 125. This way, the
values near the beginning of the vector have close to a 60/40 chance of being
less than 0.6, but by the end, they are almost certainly all above 0.6.
Take a look&nbsp;below.</p>
<p>The multiplicative factor:
<span class="caps">FYI</span>, the <code>str</code> function is a method of previewing the first few values of
variables without seeing everything at once. It can be handy. Another method is
<code>head</code>, which like the *<span class="caps">NIX</span> head, shows the first few values, but it can
be annoying if you have a matrix with a large number of columns, say over 100. Then
you get the first 10 values for all 100 columns, which takes up a lot of space. <code>str</code>
can be more convenient in that&nbsp;case.</p>
<div class="highlight"><pre><span class="o">&gt;</span> <span class="n">str</span><span class="p">(</span><span class="mi">125</span><span class="o">/</span><span class="mi">125</span><span class="o">:</span><span class="mi">1</span><span class="p">)</span>
 <span class="n">num</span> <span class="p">[</span><span class="mi">1</span><span class="o">:</span><span class="mi">125</span><span class="p">]</span> <span class="mi">1</span> <span class="mf">1.01</span> <span class="mf">1.02</span> <span class="mf">1.02</span> <span class="mf">1.03</span> <span class="p">...</span>
</pre></div>


<p>Randomly generated values in the uniform distribution:
(Notice that I re-set the random number seed. This is because after
a random seed is used, the random number generator advances to the next&nbsp;seed.)</p>
<div class="highlight"><pre><span class="o">&gt;</span> <span class="n">set</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">100</span><span class="p">);</span> <span class="n">str</span><span class="p">(</span><span class="n">runif</span><span class="p">(</span><span class="mi">125</span><span class="p">))</span>
 <span class="n">num</span> <span class="p">[</span><span class="mi">1</span><span class="o">:</span><span class="mi">125</span><span class="p">]</span> <span class="mf">0.3078</span> <span class="mf">0.2577</span> <span class="mf">0.5523</span> <span class="mf">0.0564</span> <span class="mf">0.4685</span> <span class="p">...</span>
</pre></div>


<p>Multiplicative factor times the uniform distribution vector:
(For those Linear Algebra lovers out there, we don&#8217;t get a matrix because by 
default, R multiplies vectors on an element-by-element basis. If you want
the usual vector product, aka the cross product, do <code>vector1 %*% vector2</code>)</p>
<div class="highlight"><pre><span class="o">&gt;</span> <span class="n">set</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">100</span><span class="p">);</span> <span class="n">str</span><span class="p">(</span><span class="n">runif</span><span class="p">(</span><span class="mi">125</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">125</span><span class="o">/</span><span class="mi">125</span><span class="o">:</span><span class="mi">1</span><span class="p">))</span>
 <span class="n">num</span> <span class="p">[</span><span class="mi">1</span><span class="o">:</span><span class="mi">125</span><span class="p">]</span> <span class="mf">0.3078</span> <span class="mf">0.2598</span> <span class="mf">0.5613</span> <span class="mf">0.0578</span> <span class="mf">0.484</span> <span class="p">...</span>
</pre></div>


<p>Now we can use <code>ifelse</code> to assign a 0 or 1 based on whether the vector we just
created is greater than 0.6 or&nbsp;not.</p>
<div class="highlight"><pre><span class="n">set</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">100</span><span class="p">);</span> <span class="n">classification</span><span class="p">[</span><span class="mi">76</span><span class="o">:</span><span class="mi">200</span><span class="p">]</span> <span class="o">=</span> <span class="n">ifelse</span><span class="p">(</span><span class="n">runif</span><span class="p">(</span><span class="mi">125</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">125</span><span class="o">/</span><span class="mi">125</span><span class="o">:</span><span class="mi">1</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>


<h2>The final binary&nbsp;vector</h2>
<p>This is what I get for the binary classification vector. Keep in mind that we only 
added 1&#8217;s to the last 125 values, or from index 76 and&nbsp;up.</p>
<div class="highlight"><pre><span class="o">&gt;</span> <span class="n">classification</span>
  <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span>
 <span class="p">[</span><span class="mi">38</span><span class="p">]</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span>
 <span class="p">[</span><span class="mi">75</span><span class="p">]</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span>
<span class="p">[</span><span class="mi">112</span><span class="p">]</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span>
<span class="p">[</span><span class="mi">149</span><span class="p">]</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">1</span>
<span class="p">[</span><span class="mi">186</span><span class="p">]</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span>
</pre></div>


<h1>Step 2: Get some continuous&nbsp;data.</h1>
<p>We&#8217;re going to make up continuous data similarly to how we created the binary 
classification vector. This time, we set the next seed so we&#8217;d get different 
numbers, and multiplied a uniform distribution by a linear function. Now this isn&#8217;t
quite enough to make most of the 1&#8217;s be in the upper numbers and most of the 0&#8217;s
be in the lower numbers, so we add a linearly increasing amount of Gaussian noise
to the&nbsp;values.</p>
<div class="highlight"><pre><span class="n">set</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">101</span><span class="p">);</span> <span class="n">continuous</span> <span class="o">=</span> <span class="n">runif</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span><span class="o">*</span><span class="mi">1</span><span class="o">:</span><span class="mi">200</span><span class="o">+</span><span class="n">rnorm</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span><span class="n">mean</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span><span class="n">sd</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="o">*</span><span class="mi">1</span><span class="o">:</span><span class="mi">200</span>
</pre></div>


<p>My continuous vector looks like&nbsp;this:</p>
<div class="highlight"><pre><span class="o">&gt;</span> <span class="n">str</span><span class="p">(</span><span class="n">continuous</span><span class="p">)</span>
 <span class="n">num</span> <span class="p">[</span><span class="mi">1</span><span class="o">:</span><span class="mi">200</span><span class="p">]</span> <span class="mf">51.7</span> <span class="mf">94.2</span> <span class="mf">184.1</span> <span class="mf">226.1</span> <span class="mf">269.9</span> <span class="p">...</span>
</pre></div>


<p>As you can see, we have values larger than 200, but we assumed there are 200 pieces of <em>Star Trek: <span class="caps">TNG</span></em> video media to watch, so we need to normalize it by the max. Now our maximum value is 1, so we multiply by 200 to get a maximum value of&nbsp;200.</p>
<div class="highlight"><pre><span class="o">&gt;</span> <span class="n">continuous</span> <span class="o">=</span> <span class="n">continuous</span><span class="o">/</span><span class="n">max</span><span class="p">(</span><span class="n">continuous</span><span class="p">)</span> <span class="o">*</span> <span class="mi">200</span>
</pre></div>


<h2>What does our data look&nbsp;like?</h2>
<p>Now if we plot the continuous vector vs the binary vector, we&#8217;ll see that in general,
the 0&#8217;s occur in the lower end of the continuous spectrum, and the 1&#8217;s occur in the
higher end of the continuous&nbsp;spectrum.</p>
<!-- fix data because continuous goes up to 10,000 -->

<div class="highlight"><pre><span class="n">png</span><span class="p">(</span><span class="s">&quot;~/workspace/tutorials/logistic_regression_points.png&quot;</span><span class="p">)</span>
<span class="n">plot</span><span class="p">(</span><span class="n">continuous</span><span class="p">,</span> <span class="n">classification</span><span class="p">,</span> <span class="n">pch</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;blue&quot;</span><span class="p">)</span>
<span class="n">dev</span><span class="p">.</span><span class="n">off</span><span class="p">()</span>
</pre></div>


<p><img alt="The continuous vector on the x-axis vs the binary vector in the y-axis. Notice that most of the values classified as 0 appear with lower continuous values, and the values classified as 1 appear with higher continuous values." src="http://media.tumblr.com/tumblr_mdorcfiIYs1rw6gvj.png" /></p>
<p>So according to our data, there&#8217;s a lot of people who have seen <em>most</em> of <em>Star Trek:<span class="caps">TNG</span></em> and have been to a convention, but there are a few individuals who have seen most of it and yet have not been a&nbsp;convention.</p>
<h1>Step 3: Do logistic&nbsp;regression!</h1>
<p>Now you can use generalized linear models to fit to the data. Note: This is one of many methods in R to do logistic regression, and I chose to do the simplest version. Note that <code>classification ~ continuous</code> is shorthand for <code>y ~ x</code>, or as the R people say, <code>response ~ terms</code>. If you had more data points, like Klingon language profiency, you would add this variable by using the <code>cbind</code> or &#8220;column bind&#8221; function which would create a 2-column matrix from the two vectors: <code>glm(classification ~ cbind(continuous, klingon), ...)</code>.</p>
<div class="highlight"><pre><span class="nx">glm</span><span class="p">.</span><span class="nx">out</span> <span class="o">=</span> <span class="nx">glm</span><span class="p">(</span><span class="nx">classification</span> <span class="o">~</span> <span class="nx">continuous</span><span class="p">,</span> <span class="nx">family</span><span class="o">=</span><span class="nx">binomial</span><span class="p">(</span><span class="nx">logit</span><span class="p">))</span>
<span class="nx">png</span><span class="p">(</span><span class="s2">&quot;~/workspace/tutorials/logistic_fitted.png&quot;</span><span class="p">)</span>
<span class="nx">plot</span><span class="p">(</span><span class="nx">continuous</span><span class="p">,</span> <span class="nx">glm</span><span class="p">.</span><span class="nx">out$fitted</span><span class="p">,</span> <span class="nx">type</span><span class="o">=</span><span class="s2">&quot;l&quot;</span><span class="p">,</span> <span class="nx">col</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="nx">lwd</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="nx">main</span><span class="o">=</span><span class="s2">&quot;Logistic function fitted to classification data\nvia a generalized linear model&quot;</span><span class="p">)</span>
<span class="nx">points</span><span class="p">(</span><span class="nx">continuous</span><span class="p">,</span> <span class="nx">classification</span><span class="p">,</span> <span class="nx">col</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="nx">pch</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="nx">dev</span><span class="p">.</span><span class="nx">off</span><span class="p">()</span>
</pre></div>


<p>The plot of the fitted curve looks&nbsp;like:</p>
<p><img alt="Logistic curve fitted to data." src="http://media.tumblr.com/tumblr_mdorcxMktg1rw6gvj.png" /></p>
<h2>So what actually happened&nbsp;here?</h2>
<p>The <code>glm</code> function found the values of $a_0$ and $a_1$ that minimize the error term. I&#8217;m not qualified to describe <code>glm</code> in gory detail, or all the other possible methods of calculating logistic regression, but <a href="http://www.omidrouhani.com/research/logisticregression/html/logisticregression.htm#_Toc147483467">this site</a> gives the mathematics the justice it deserves. You will need to be familiar with linear algebra to fully understand the&nbsp;arguments.</p>
<h1><span class="caps">FAQ</span></h1>
<h2>What if your classification is 1 for the low values and 0 for the high&nbsp;values?</h2>
<p>You can still use the same steps, but your logit curve will be flipped along the&nbsp;$y$-axis.</p>
<h2>Have you attended a <em>Star Trek</em>&nbsp;convention?</h2>
<p>No, I have not. But I thought it would be a fun binary&nbsp;variable.</p>
<h1>References</h1>
<p>I could not have done this without the help of many different&nbsp;sources.</p>
<ul>
<li>http://luna.cas.usf.edu/~mbrannic/files/regression/Logistic.html - the first thing I read, and the primary source for the explanation of logistic regression in the first section (<em>What in Merlin&#8217;s beard is logistic regression?</em>). Highly recommended introduction to logistic&nbsp;regression.</li>
<li>http://en.wikipedia.org/wiki/Logistic_regression - Wikipedia is a great place to start, but it dove into the technical aspects a little too quickly for my&nbsp;taste.</li>
<li>http://www.statgun.com/tutorials/logistic-regression.html - a great explanation of logistic regression in&nbsp;general.</li>
<li>http://ww2.coastal.edu/kingw/statistics/R-tutorials/logistic.html - used R to explain logistic regression of a particular&nbsp;dataset.</li>
<li>http://www.omidrouhani.com/research/logisticregression/html/logisticregression.htm - mathematical derivation of logistic&nbsp;regression</li>
<li>http://ww2.coastal.edu/kingw/statistics/R-tutorials/logistic.html - explains logistic regression and odds, and implements logistic regression on a particular&nbsp;dataset.</li>
</ul>
            <a href="#" class="go-top">Go Top</a>
    <div class="comments">
        <div id="disqus_thread"></div>
        <script type="text/javascript">
            /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
            var disqus_shortname = "sciencemeetproductivity"; // required: replace example with your forum shortname

            /* * * DON'T EDIT BELOW THIS LINE * * */
            (function() {
                var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
        <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    </div>
<footer class="footer">
    <p>&copy; Olga Botvinnik &ndash;
        Built with <a href="https://github.com/PurePelicanTheme/pure-single">Pure Theme</a>
        for <a href="http://blog.getpelican.com/">Pelican</a>
    </p>
</footer>        </div>
    </div>
    </div>
    <script>
        var $top = $('.go-top');

        // Show or hide the sticky footer button
        $(window).scroll(function() {
            if ($(this).scrollTop() > 200) {
                $top.fadeIn(200);
            } else {
                $top.fadeOut(200);
            }
        });

        // Animate the scroll to top
        $top.click(function(event) {
            event.preventDefault();
            $('html, body').animate({scrollTop: 0}, 300);
        })

        // Makes sure that the href="#" attached to the <a> elements
        // don't scroll you back up the page.
        $('body').on('click', 'a[href="#"]', function(event) {
            event.preventDefault();
        });
    </script>
    <script type="text/javascript">
        var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
        document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
    </script>
    <script type="text/javascript">
        try {
            var pageTracker = _gat._getTracker("UA-53680167-1");
            pageTracker._trackPageview();
            } catch(err) {}
    </script>

</body>
</html>